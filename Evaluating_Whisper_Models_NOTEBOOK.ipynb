{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8cJCToxNeN5",
        "outputId": "f7d3ff34-1e72-4d68-8cd0-37b680c00fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 28 15:51:32 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0              50W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-VPTXDbaOrKM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install datasets torchaudio librosa huggingface_hub jiwer\n",
        "!pip install https://github.com/kpu/kenlm/archive/master.zip\n",
        "!pip install pyctcdecode\n",
        "!pip install bitsandbytes\n",
        "!pip install evaluate\n",
        "!pip install wandb\n",
        "!pip install jiwer\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C5Q_NvwK0uG9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "af078770b505498887956e2dfb848f75",
            "c0a46558d79049529325bd253a38cc55"
          ]
        },
        "id": "TEPXo4koOuY4",
        "outputId": "3db802e4-ed37-4f7c-c6cb-2fb09184e64e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af078770b505498887956e2dfb848f75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pUFto5IrQOPW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L118BqAPfG6",
        "outputId": "86374c56-dd8a-45f3-d24c-3f8e098f02c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bigcgen'...\n",
            "remote: Enumerating objects: 35681, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 35681 (delta 1), reused 8 (delta 1), pack-reused 35671 (from 4)\u001b[K\n",
            "Receiving objects: 100% (35681/35681), 7.40 GiB | 18.05 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "Updating files: 100% (35640/35640), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/csikasote/bigcgen.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9uRhW2ozUzus"
      },
      "outputs": [],
      "source": [
        "audio_path = os.path.join(os.getcwd(),\"bigcgen/audio/\")\n",
        "csv_path = os.path.join(os.getcwd(),\"bigcgen/splits/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "23eDu0d4U0b9"
      },
      "outputs": [],
      "source": [
        "def remove_processed_files(csv_path):\n",
        "  processed_files = glob(f\"{csv_path}*/*_processed.tsv\")\n",
        "  len(processed_files)\n",
        "  for f in processed_files:\n",
        "    os.remove(f)\n",
        "  print(\"Processed files removed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUNQWHpkVl0u",
        "outputId": "6c966c94-4351-4d6d-bfdf-278e6265d570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed files removed successfully\n"
          ]
        }
      ],
      "source": [
        "remove_processed_files(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3Lr49AdZVxjx"
      },
      "outputs": [],
      "source": [
        "def prepare_data(audio_path, csv_path):\n",
        "    split_list = [\"male\",\"female\",\"balanced\", \"combined\", \"test\"]\n",
        "    for split in split_list:\n",
        "        csv_file_list = glob(f\"{csv_path}{split}/*.tsv\")\n",
        "        for csv_file in csv_file_list:\n",
        "            split_file = os.path.basename(csv_file).split(\".\")[0]\n",
        "            df = pd.read_csv(csv_file, sep=\"\\t\")\n",
        "            df[\"path\"] = audio_path + df['audio']\n",
        "            df = df.dropna(subset=[\"path\"])\n",
        "            df = df.drop(columns=['audio'])\n",
        "            df = df.rename(columns={'path':'audio'})\n",
        "            df = df[[\"audio\",\"sentence\"]]\n",
        "            df.to_csv(f\"{csv_path}/{split}/{split_file}_processed.tsv\", sep=\"\\t\", index=False)\n",
        "            print(f\"{split_file}_processed : \", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f_oneway\n",
        "def compute_OneWayANOVA():\n",
        "  male_df = pd.read_csv(f\"male_wer.csv\")\n",
        "  female_df = pd.read_csv(f\"female_wer.csv\")\n",
        "  male_list = male_df.values.tolist()\n",
        "  female_list = female_df.values.tolist()\n",
        "  anova_results = f_oneway(male_list, female_list)\n",
        "  sig_value = ''\n",
        "  if anova_results[1][0] < 0.05:\n",
        "    sig_value = 'True'\n",
        "  else:\n",
        "    sig_value = 'False'\n",
        "  print(\"One-Way ANOVA:\")\n",
        "  print(\"F Statistic:\",anova_results[0])\n",
        "  print(\"P value:\",anova_results[1], \":Significant:\",sig_value)"
      ],
      "metadata": {
        "id": "7yX9CSWNkPz-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN_FZb0fVx1T",
        "outputId": "8b1436b4-2289-47c6-c3dd-2d75c8f32854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_male_20hrs_file_processed :  10284\n",
            "train_male_10hrs_file_processed :  5171\n",
            "validation_male_file_processed :  441\n",
            "test_native_and_nonnative_male_file_processed :  461\n",
            "train_male_5hrs_file_processed :  2577\n",
            "train_male_15hrs_file_processed :  7749\n",
            "train_male_30hrs_file_processed :  15463\n",
            "train_male_25hrs_file_processed :  12857\n",
            "train_female_30hrs_file_processed :  16036\n",
            "train_female_15hrs_file_processed :  8008\n",
            "train_female_25hrs_file_processed :  13347\n",
            "train_female_10hrs_file_processed :  5352\n",
            "validation_female_file_processed :  475\n",
            "train_female_20hrs_file_processed :  10674\n",
            "train_female_5hrs_file_processed :  2675\n",
            "test_native_and_nonnative_female_file_processed :  472\n",
            "validation_balanced_file_processed :  442\n",
            "train_balanced_file_processed :  2631\n",
            "train_combined_10hrs_file_processed :  5252\n",
            "train_combined_5hrs_file_processed :  2631\n",
            "train_combined_40hrs_file_processed :  20958\n",
            "train_combined_25hrs_file_processed :  13129\n",
            "train_combined_30hrs_file_processed :  15757\n",
            "train_combined_55hrs_file_processed :  28868\n",
            "train_combined_35hrs_file_processed :  18387\n",
            "validation_combined_file_processed :  916\n",
            "train_combined_65hrs_file_processed :  33676\n",
            "train_combined_15hrs_file_processed :  7867\n",
            "train_combined_50hrs_file_processed :  26204\n",
            "test_combined_file_processed :  933\n",
            "train_combined_20hrs_file_processed :  10523\n",
            "train_combined_45hrs_file_processed :  23584\n",
            "train_combined_60hrs_file_processed :  31499\n",
            "test_nonnative_female_file_processed :  251\n",
            "test_native_male_file_processed :  192\n",
            "test_native_female_file_processed :  221\n",
            "test_native_and_nonnative_male_file_processed :  461\n",
            "test_nonnative_male_and_female_speakers_file_processed :  520\n",
            "test_native_male_and_female_speakers_file_processed :  413\n",
            "test_native_and_nonnative_female_file_processed :  472\n",
            "test_combined_file_processed :  933\n",
            "test_nonnative_male_file_processed :  269\n"
          ]
        }
      ],
      "source": [
        "prepare_data(audio_path, csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s323Jwz0-C59",
        "outputId": "3cc753da-b160-4d71-ab89-99ba740572e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-28 15:54:06--  https://raw.githubusercontent.com/csikasote/ft_scripts/refs/heads/main/run_eval_whisper_modelv2.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5886 (5.7K) [text/plain]\n",
            "Saving to: ‘run_eval_whisper_modelv2.py’\n",
            "\n",
            "run_eval_whisper_mo 100%[===================>]   5.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-28 15:54:07 (76.0 MB/s) - ‘run_eval_whisper_modelv2.py’ saved [5886/5886]\n",
            "\n",
            "--2025-01-28 15:54:09--  https://raw.githubusercontent.com/csikasote/scripts/refs/heads/main/run_eval_whisper.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1260 (1.2K) [text/plain]\n",
            "Saving to: ‘run_eval_whisper.sh’\n",
            "\n",
            "run_eval_whisper.sh 100%[===================>]   1.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-28 15:54:10 (62.3 MB/s) - ‘run_eval_whisper.sh’ saved [1260/1260]\n",
            "\n",
            "--2025-01-28 15:54:10--  https://raw.githubusercontent.com/csikasote/scripts/refs/heads/main/compute_ANOVA.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 605 [text/plain]\n",
            "Saving to: ‘compute_ANOVA.py’\n",
            "\n",
            "compute_ANOVA.py    100%[===================>]     605  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-28 15:54:10 (35.3 MB/s) - ‘compute_ANOVA.py’ saved [605/605]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the CODE from\n",
        "!wget https://raw.githubusercontent.com/csikasote/ft_scripts/refs/heads/main/run_eval_whisper_modelv2.py\n",
        "!wget https://raw.githubusercontent.com/csikasote/scripts/refs/heads/main/run_eval_whisper.sh\n",
        "!wget https://raw.githubusercontent.com/csikasote/scripts/refs/heads/main/compute_ANOVA.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1PzQF6HWD6h"
      },
      "outputs": [],
      "source": [
        "model_id=\"csikasote/whisper-medium-bemgen-combined-model\"\n",
        "dataset=\"bemgen\"\n",
        "split=\"female\"\n",
        "file_name=\"combined\"\n",
        "csv_test_path = f\"/content/{dataset}/splits/{split}/test_{file_name}_file_processed.tsv\"\n",
        "output_file = f\"/content/{split}_output_wer.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcGbKrVP021D",
        "outputId": "b2b99bd1-f0ea-4426-9fdf-4d372825e1b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-01-27 09:26:51.302604: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-01-27 09:26:51.319295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-27 09:26:51.341082: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-27 09:26:51.347459: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-27 09:26:51.363026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-27 09:26:52.603550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 4.49k/4.49k [00:00<00:00, 17.0MB/s]\n",
            "config.json: 100% 1.27k/1.27k [00:00<00:00, 9.05MB/s]\n",
            "model.safetensors: 100% 3.06G/3.06G [02:05<00:00, 24.4MB/s]\n",
            "generation_config.json: 100% 3.70k/3.70k [00:00<00:00, 26.8MB/s]\n",
            "tokenizer_config.json: 100% 283k/283k [00:00<00:00, 26.7MB/s]\n",
            "vocab.json: 100% 836k/836k [00:00<00:00, 52.7MB/s]\n",
            "tokenizer.json: 100% 3.93M/3.93M [00:01<00:00, 3.60MB/s]\n",
            "merges.txt: 100% 494k/494k [00:00<00:00, 1.15MB/s]\n",
            "normalizer.json: 100% 52.7k/52.7k [00:00<00:00, 153MB/s]\n",
            "added_tokens.json: 100% 34.6k/34.6k [00:00<00:00, 15.0MB/s]\n",
            "special_tokens_map.json: 100% 2.19k/2.19k [00:00<00:00, 16.6MB/s]\n",
            "preprocessor_config.json: 100% 339/339 [00:00<00:00, 2.58MB/s]\n",
            "Device set to use cuda:0\n",
            "Generating test split: 505 examples [00:00, 32507.50 examples/s]\n",
            "Map: 100% 505/505 [00:00<00:00, 12603.08 examples/s]\n",
            "Filter: 100% 505/505 [00:00<00:00, 60259.56 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "WER: 30.4\n",
            "505 505\n",
            "Successfully saved the list of WER\n"
          ]
        }
      ],
      "source": [
        "!python run_eval_whisper_modelv2.py \\\n",
        "  --model_id=$model_id \\\n",
        "  --dataset=$dataset \\\n",
        "  --config=\"en\" \\\n",
        "  --streaming=\"False\" \\\n",
        "  --path=$csv_test_path \\\n",
        "  --output=$output_file \\\n",
        "  --device=0 \\\n",
        "  --language=\"en\" \\\n",
        "  --batch_size=8 \\\n",
        "  --task=\"transcribe\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkiz2PqjH8ld",
        "outputId": "91d4156b-0200-4c68-e5fa-8b1fcbac8d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "male\n",
            "2025-01-28 17:45:33.720761: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-01-28 17:45:33.739083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-28 17:45:33.760264: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-28 17:45:33.766847: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-28 17:45:33.782477: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-28 17:45:34.924193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Device set to use cuda:0\n",
            "Generating test split: 461 examples [00:00, 82374.39 examples/s]\n",
            "Map: 100% 461/461 [00:00<00:00, 14292.39 examples/s]\n",
            "Filter: 100% 461/461 [00:00<00:00, 129648.26 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "WER: 198.57\n",
            "461 461\n",
            "Successfully saved the list of WER\n",
            "female\n",
            "2025-01-28 17:53:00.801132: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-01-28 17:53:00.818796: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-28 17:53:00.839874: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-28 17:53:00.846377: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-28 17:53:00.862047: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-28 17:53:02.052877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Device set to use cuda:0\n",
            "Generating test split: 472 examples [00:00, 77910.72 examples/s]\n",
            "Map: 100% 472/472 [00:00<00:00, 13767.50 examples/s]\n",
            "Filter: 100% 472/472 [00:00<00:00, 122711.93 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "WER: 200.91\n",
            "472 472\n",
            "Successfully saved the list of WER\n",
            "combined\n",
            "2025-01-28 17:59:43.458123: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-01-28 17:59:43.475884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-28 17:59:43.497362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-28 17:59:43.503941: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-28 17:59:43.519372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-28 17:59:44.757070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Device set to use cuda:0\n",
            "Generating test split: 933 examples [00:00, 134287.97 examples/s]\n",
            "Map: 100% 933/933 [00:00<00:00, 15402.03 examples/s]\n",
            "Filter: 100% 933/933 [00:00<00:00, 202195.19 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "WER: 199.76\n",
            "933 933\n",
            "Successfully saved the list of WER\n",
            "\n",
            "One-Way ANOVA:\n",
            "F Statistic: [0.05896126]\n",
            "P value: [0.8081988] :Significant: False\n"
          ]
        }
      ],
      "source": [
        "# Run the model\n",
        "!bash run_eval_whisper_single_model.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBgKIX14H80S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "af078770b505498887956e2dfb848f75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_c0a46558d79049529325bd253a38cc55"
          }
        },
        "c0a46558d79049529325bd253a38cc55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}